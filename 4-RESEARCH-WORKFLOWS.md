# ğŸ”¬ Phase 4: Research Workflows

Goal: Turn complex questions into rigorous multiâ€‘step research with predictable outputs, while preserving speed where possible.

## ğŸ¯ Definition of Done
- Planner consistently produces a brief, useful plan (subtopics + questions).
- Gatherer alternates KB + web, then explicitly declares readiness.
- Report Builder emits a wellâ€‘structured markdown report with inline citations and a references section.
- Loops are bounded; failures are surfaced clearly.
- Evaluations in place for routing prompt - 3 minimum
- Context compression: keep last ~10 turns with a running summary; perâ€‘source summaries within a token budget
  - Keep last N turns verbatim (Nâ‰ˆ8â€“12); fold older turns into a running summary
  - On overflow, update the summary and drop the oldest verbatim turn

## ğŸ§± Roles & Responsibilities
- Planner
  - Extract subtopics and key questions.
  - Identify which ones require web vs. KB.
  - Produce a minimal, actionable plan (not an essay).
- Gatherer (loop)
  - Follow the plan; collect KB first, then web.
  - Summarize findings briefly; call out gaps.
  - When coverage is sufficient, write: â€œRESEARCH COMPLETE â€” ready to build reportâ€.
- Report Builder
  - Title, sections, inline citations ([KBâ€‘X], [WEBâ€‘X]), and a references section.
  - Synthesis over aggregation; avoid bullet dumps.

## ğŸ” Loop Guardrails
- Soft cap on gatherer iterations (e.g., 3â€“5) with a short justification if exceeded.
- Prefer breadth over depth after the second pass (reduce duplicative searches).
- Fail fast if no sources are found after the first pass (signal to user).

## ğŸ§  Context Compression & Token Budgets
Keep the research context within model limits without losing traceability.

- Token management: avoid sending full history. Keep the last ~10 turns and roll older turns into a running summary that is included with the recent context.
- Large result sets: summarize retrieved content as perâ€‘source notes before synthesis to prevent oversized contexts.

## ğŸ§ª Routing Evaluation & Prompt Tuning
Routing is trickyâ€”plan to iterate. I strongly advise you add a lightweight eval loop to tune the router:
- Start by vibing one for review - it really helps get there quickly
- Tune the prompt using concrete indicators (multiâ€‘section asks, comparisons, â€œcomprehensive/report/deep diveâ€, etc.).
- Return structured JSON from the router (e.g., decision, confidence, rationale, indicators) to reduce ambiguity and improve parsing.
- Treat ambiguous requests as simple by default unless strong research indicators are present.
- Reâ€‘run evals after each prompt change

## ğŸ“Œ Quality Bar for Reports
- Structure: Introduction, Core Concepts, Current State, Challenges, Future Trends, Applications, Conclusion, References.
- Citations: Inline for claims; references at end split by KB vs. Web.
- Substance: Explanatory paragraphs, not lists of quotes.
- Length: Aim for ~800+ words when asked for "comprehensive".

## âœ… Acceptance Tests
- It correcly produces a research report when asked or a simple chat experince. 
- The UI clearly indicates the type of action taken and sources are properly cited in the response. 

